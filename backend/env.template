# SRE AI Agent - Environment Configuration Template
# Copy this file to .env and fill in your actual values

# =============================================================================
# REQUIRED: LLM Provider API Keys
# =============================================================================
# At least one LLM provider API key is required for the agent to function

# Google Gemini API Key
# Get from: https://makersuite.google.com/app/apikey or https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# OpenAI API Key  
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Grok API Key (X.AI)
# Get from: https://console.x.ai/
GROK_API_KEY=your_grok_api_key_here

# =============================================================================
# REQUIRED: GitHub Configuration
# =============================================================================
# Required for downloading runbooks from repositories
# Get from: https://github.com/settings/tokens
# Permissions needed: repo (for private repos) or public_repo (for public repos)
GITHUB_TOKEN=your_github_token_here

# =============================================================================
# Multi-Layer Agent Architecture Configuration
# =============================================================================
# The agent registry and MCP server registry are configured in settings.py
# with sensible defaults. You can override them here if needed.

# Agent Registry - Maps alert types to specialized agent classes
# Example override (JSON format):
# AGENT_REGISTRY='{"Namespace is stuck in Terminating": "KubernetesAgent", "ArgoCD Sync Failed": "ArgoCDAgent"}'

# MCP Server Registry - Single source of truth for all MCP server configurations
# Note: This replaces the old mcp_servers configuration
# Example override (JSON format):
# MCP_SERVER_REGISTRY='{"kubernetes-server": {"server_type": "kubernetes", "enabled": true, "connection_params": {"command": "npx", "args": ["-y", "@modelcontextprotocol/server-kubernetes@latest"]}, "instructions": "For Kubernetes operations..."}}'

# =============================================================================
# Application Settings
# =============================================================================
# Default LLM provider to use (gemini, openai, or grok)
DEFAULT_LLM_PROVIDER=openai

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Server host and port configuration
HOST=0.0.0.0
PORT=8000

# =============================================================================
# CORS Configuration
# =============================================================================
# Allowed origins for CORS (comma-separated list)
# For development, include your frontend URL
# For production, use your actual domain
CORS_ORIGINS=http://localhost:3001,http://127.0.0.1:3001

# =============================================================================
# QUICK START INSTRUCTIONS
# =============================================================================
# 
# 1. Copy this file to .env:
#    cp env.template .env
#
# 2. Edit .env and set these REQUIRED variables:
#    - At least one LLM API key (GEMINI_API_KEY, OPENAI_API_KEY, or GROK_API_KEY)
#    - GITHUB_TOKEN for runbook access
#
# 3. Optional: Set up Kubernetes MCP Server on localhost:8080
#    Or use the mock server script in DEPLOYMENT.md
#
# 4. Start the backend:
#    uvicorn app.main:app --reload --port 8000
#
# =============================================================================

# =============================================================================
# Optional: Advanced Configuration
# =============================================================================

# LLM Model Configuration
# GEMINI_MODEL=gemini-2.5-pro
# OPENAI_MODEL=gpt-4
# GROK_MODEL=grok-3

# Request Timeouts (seconds)
# REQUEST_TIMEOUT=30
# LLM_TIMEOUT=60
# MCP_TIMEOUT=30

# Processing Limits
# MAX_CONCURRENT_ALERTS=5
# MAX_RUNBOOK_SIZE_MB=10

# GitHub API Configuration
# GITHUB_API_URL=https://api.github.com
# GITHUB_RAW_URL=https://raw.githubusercontent.com

# WebSocket Configuration
# WS_PING_INTERVAL=20
# WS_PING_TIMEOUT=10
# WS_MAX_CONNECTIONS=100

# =============================================================================
# Development/Testing Settings
# =============================================================================

# Enable verbose debug logging
# DEBUG_MODE=false

# Use mock responses for testing without real services
# USE_MOCK_MCP=false
# USE_MOCK_LLM=false

# Cache Configuration
# RUNBOOK_CACHE_TTL=3600
# RUNBOOK_CACHE_DIR=/tmp/sre-runbooks
# ENABLE_RESPONSE_CACHE=true

# =============================================================================
# Production Settings
# =============================================================================

# Security
# SECRET_KEY=your-super-secret-key-for-production
# ALLOWED_HOSTS=your-domain.com,api.your-domain.com

# Database (optional - for persistent storage)
# DATABASE_URL=postgresql://user:password@localhost:5432/sre_agent

# Redis (optional - for caching and task queues)  
# REDIS_URL=redis://localhost:6379/0

# SSL/TLS Configuration
# SSL_KEYFILE=/path/to/ssl/private.key
# SSL_CERTFILE=/path/to/ssl/certificate.crt

# Rate Limiting
# RATE_LIMIT_ENABLED=true
# RATE_LIMIT_PER_MINUTE=60
# RATE_LIMIT_BURST=10

# Monitoring and Metrics
# ENABLE_METRICS=true
# METRICS_PORT=9090
# HEALTH_CHECK_INTERVAL=30

# =============================================================================
# Example Test Values
# =============================================================================
# Use these for initial testing and development

# Example cluster URL (replace with your actual cluster)
# TEST_CLUSTER_URL=https://api.example-cluster.com:6443

# Example namespace
# TEST_NAMESPACE=kube-system

# Example alert message
# TEST_ALERT_MESSAGE=namespace is stuck in 'Terminating' phase

# Default test runbook
# TEST_RUNBOOK_URL=https://github.com/codeready-toolchain/sandbox-sre/blob/master/runbooks/namespace-terminating.md

# =============================================================================
# Environment-Specific Configuration
# =============================================================================
# You can create multiple environment files:
# - .env.development
# - .env.staging
# - .env.production
#
# Load specific environment with:
# uvicorn app.main:app --env-file .env.production

# =============================================================================
# Troubleshooting
# =============================================================================
# 
# Common issues and solutions:
#
# 1. "LLM provider not available" error:
#    - Check that at least one API key is set correctly
#    - Verify the API key has sufficient quota/credits
#    - Test the API key with a simple request
#
# 2. "Failed to download runbook" error:
#    - Check GITHUB_TOKEN has correct permissions
#    - Verify the runbook URL is accessible
#    - Test with a public repository first
#
# 3. "MCP server not available" error:
#    - Check KUBERNETES_MCP_URL is correct
#    - Verify the MCP server is running and responding
#    - Use the mock server script for testing
#
# 4. Frontend connection issues:
#    - Check CORS_ORIGINS includes your frontend URL
#    - Verify backend is running on the correct port
#    - Check browser developer console for CORS errors 

# Additional LLM API Keys
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AI...
X_AI_API_KEY=xai-... 